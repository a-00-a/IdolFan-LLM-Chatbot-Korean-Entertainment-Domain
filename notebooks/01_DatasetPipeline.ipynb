{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOBqpsd7PYgKJjPQtfG+LLG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/a-00-a/IdolFan-LLM-Chatbot-Korean-Entertainment-Domain/blob/main/notebooks/01_DatasetPipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 01_DatasetPipeline.ipynb\n",
        "from datasets import Dataset\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# ì˜ˆì‹œ ë°ì´í„°\n",
        "sample_data = [\n",
        "    {\"prompt\": \"ì˜¤ëŠ˜ í•˜ë£¨ ì–´ë• ì–´ìš”?\", \"completion\": \"íŒ¬ë“¤ ìƒê°í•˜ë©´ì„œ í˜ëƒˆì–´ìš”! ğŸ˜Š\"},\n",
        "    {\"prompt\": \"ì¶”ì²œ ë…¸ë˜ ìˆì–´ìš”?\", \"completion\": \"ì œ ìµœì•  ë…¸ë˜ëŠ” 'Shakira-Zoo'ì—ìš”!ğŸµ\"}\n",
        "]\n",
        "\n",
        "def prepare_dataset(data_list, model_name=\"skt/kogpt2-base-v2\"):\n",
        "    dataset = Dataset.from_list(data_list)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    tokenizer.pad_token = tokenizer.eos_token # Add this line to set the padding token\n",
        "\n",
        "    def tokenize(batch):\n",
        "        # batch ë‹¨ìœ„ë¡œ ì²˜ë¦¬\n",
        "        return tokenizer(\n",
        "            batch[\"prompt\"],\n",
        "            batch[\"completion\"],\n",
        "            truncation=True,\n",
        "            padding=\"max_length\",\n",
        "            max_length=128\n",
        "        )\n",
        "\n",
        "    tokenized_dataset = dataset.map(tokenize, batched=True)\n",
        "    tokenized_dataset = tokenized_dataset.remove_columns([\"prompt\", \"completion\"])\n",
        "    tokenized_dataset.set_format(\"torch\")\n",
        "\n",
        "    return tokenized_dataset, tokenizer\n",
        "\n",
        "tokenized_dataset, tokenizer = prepare_dataset(sample_data)\n",
        "print(tokenized_dataset)"
      ],
      "metadata": {
        "id": "LhYeFckNQPDT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
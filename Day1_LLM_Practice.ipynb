{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1esM6ydp5BczkpeZQVUdM4NVYQStoSvK3",
      "authorship_tag": "ABX9TyPEaY8PNoM5dVnqZRzPeErM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/a-00-a/LLM_Practice/blob/main/Day1_LLM_Practice.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sv-qgtHrjX3r",
        "outputId": "8d680e39-c5c2-451d-cb5a-7393cb660124"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "# GPU 실행 확인 -> True 나오면 GPU 사용 가능\n",
        "import torch\n",
        "torch.cuda.is_available()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 라이브러리 설치\n",
        "!pip install transformers datasets torch gradio --quiet"
      ],
      "metadata": {
        "id": "qqav1IBblfYV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y0SShTGaxcLG",
        "outputId": "833d5a1c-ac8a-4f92-eb62-3ff1a05b563d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers, datasets, torch, gradio\n",
        "print(transformers.__version__)\n",
        "print(torch.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rJXTbiF8l5MG",
        "outputId": "433c49e4-d82a-4e88-fb01-5993d4644204"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4.57.6\n",
            "2.9.0+cu126\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# GPU 사용 가능 확인\n",
        "import torch\n",
        "print(\"GPU 사용 가능 여부:\", torch.cuda.is_available())\n",
        "print(\"사용 가능한 GPU 이름:\", torch.cuda.get_device_name(0)\n",
        "if torch.cuda.is_available() else \"GPU 없음\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "elEmNxsBmWfR",
        "outputId": "111d0e4d-f21b-4741-a094-174d37f0404f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU 사용 가능 여부: True\n",
            "사용 가능한 GPU 이름: Tesla T4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# HuggingFace로 GPT-2 문장 생성\n",
        "from transformers import pipeline\n",
        "\n",
        "generator = pipeline(\"text-generation\", model=\"gpt2\")\n",
        "prompt = \"Once upon a time in a magical theater,\"\n",
        "result = generator(\n",
        "    prompt,\n",
        "    max_length=50,\n",
        "    num_return_sequences=1,\n",
        "    do_sample=True, # 샘플링 활성화 -> 무작위 샘플링\n",
        "    top_k=50, # 상위 50개 후보에서 선택 -> 반복 패턴 줄이고 다양하게 생\n",
        "    top_p=0.95, # 누적 확률 95% 안에서 선택 -> 상\n",
        "    temperature=0.8 # 다양성 조절 -> 높일수록 더 창의적, 낮출수록 반복적\n",
        "    )\n",
        "print(\"\\n=== 생성된 문장 ===\")\n",
        "print(result[0]['generated_text'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hEarXyYooXJR",
        "outputId": "74217051-9bf9-48ff-a00d-1f8ffcb333c7"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== 생성된 문장 ===\n",
            "Once upon a time in a magical theater, someone must have been able to make their way to a room and see their new love. It's not always a good feeling to be at a party where your romantic partner's body is completely different than theirs. But to be able to hear her tell you the truth about what happened makes you feel more human, more real.\n",
            "\n",
            "What about my best friend?\n",
            "\n",
            "Your best friend is so amazing. It's no surprise that she is such a joy to be with you. Not only that, she's such a nice person, and you're so lucky to have her. She's a wonderful person. And she's just as good as any other woman you know.\n",
            "\n",
            "How are you going to get to know your best friend, so you can make it better for her?\n",
            "\n",
            "The truth is that your best friend will always be there for you. But you'll also always have a special place in your heart for her. And she is so important to you that she will always be there for you.\n",
            "\n",
            "Is it your fault that my best friend has been bullied?\n",
            "\n",
            "That's not really what it is. She's the one person who I have a relationship with who will always be there for me and always be there for me.\n",
            "\n"
          ]
        }
      ]
    }
  ]
}
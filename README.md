# IdolFan LLM Chatbot: Korean Entertainment Domain

## Overview
Fine-tuned open-source LLM to emulate a specific idol's personality and speech style for fan interactions in Korean.

## Motivation
Fans want to interact with idols in natural dialogue. This project demonstrates end-to-end LLM application in entertainment content.

## Dataset
-Source: Idol SNS posts, interviews, fan Q&A
-Design: Fan questions -> Idol-style responses
-Preprocessing: Text cleaning, tokenization, formatting

## Model & Training
-Base model: small Korean-capable LLM
-Fine-tuning: LoRA / PEFT
-Training setup

## Evaluation
-Baseline vs fine-tuned qualitative comparison
-Example prompts and outputs

## Deployment
-Gradio interface
-Korean language interface examples

## Notes on LLM Trends
-Recent LLM research insights
-Applicability to fan-oriented chat services

## π“‚ Day 1 : ν™κ²½ μ„¤μ •
- GPU ν™•μΈ λ° ν™κ²½ μ„Έν…
- transformer, torch, datasets import λ° λ²„μ „ ν™•μΈ
- Colab Notebook: [Day1] (https://colab.research.google.com/github/a-00-a/LLM_Practice/blob/main/day1_environment_setup.ipynb)

## π“‚ Day 2 : λ°μ΄ν„°μ…‹ νμ΄ν”„λΌμΈ
- Hugging Face Dataset λ΅λ”©
- ν…μ¤νΈ μ •μ  λ° ν† ν¬λ‚μ΄μ§•
- Colab Notebook: [Day2] (https://colab.research.google.com/github/a-00-a/LLM_Practice/blob/main/day2_dataset_pipeline.ipynb) 

# 01_DatasetPipeline.ipynb
from datasets import Dataset
from transformers import AutoTokenizer

# ì˜ˆì‹œ ë°ì´í„°
sample_data = [
    {"prompt": "ì˜¤ëŠ˜ í•˜ë£¨ ì–´ë• ì–´ìš”?", "completion": "íŒ¬ë“¤ ìƒê°í•˜ë©´ì„œ í˜ëƒˆì–´ìš”! ğŸ˜Š"},
    {"prompt": "ì¶”ì²œ ë…¸ë˜ ìˆì–´ìš”?", "completion": "ì œ ìµœì•  ë…¸ë˜ëŠ” 'Shakira-Zoo'ì—ìš”!ğŸµ"}
]

def prepare_dataset(data_list, model_name="skt/kogpt2-base-v2"):
    dataset = Dataset.from_list(data_list)
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    tokenizer.pad_token = tokenizer.eos_token # Add this line to set the padding token

    def tokenize(batch):
        # batch ë‹¨ìœ„ë¡œ ì²˜ë¦¬
        return tokenizer(
            batch["prompt"],
            batch["completion"],
            truncation=True,
            padding="max_length",
            max_length=128
        )

    tokenized_dataset = dataset.map(tokenize, batched=True)
    tokenized_dataset = tokenized_dataset.remove_columns(["prompt", "completion"])
    tokenized_dataset.set_format("torch")

    return tokenized_dataset, tokenizer

tokenized_dataset, tokenizer = prepare_dataset(sample_data)
print(tokenized_dataset)
